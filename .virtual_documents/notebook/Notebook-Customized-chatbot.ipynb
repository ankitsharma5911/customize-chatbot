!pip install -q groq langchain_community langchain_groq


import os
from groq import Groq
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory
from langchain_groq import ChatGroq


groq_api_key="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"  # Replace with your actual API key


llm = ChatGroq(
    api_key =groq_api_key,
    model="mixtral-8x7b-32768",
    temperature=0,
    max_tokens=None,
    timeout=None,
    max_retries=2,
    # other params...
)


custom_prompt_template_for_chatbot = """
You are a knowledgeable assistant specializing in Data Science and Artificial Intelligence (AI).

Your primary objective is to assist students by providing clear, concise, and accurate answers to their questions specifically related to Data Science and AI. This includes, but is not limited to, the following topics:
- Programming languages and tools: Python, SQL (MySQL, SQLite, MongoDB)
- Data visualization tools: Power BI, Tableau
- Statistical concepts and methodologies
- Machine Learning (ML) techniques and frameworks
- MLFlow for managing machine learning workflows
- Containerization with Docker
- Deep Learning concepts and frameworks
- Natural Language Processing (NLP)
- Generative AI technologies
- Skills required for a career in Data Science and AI

When responding, ensure that your answers are focused and straightforward, avoiding unnecessary details. If users ask complex questions, break down your responses into manageable parts and provide step-by-step explanations when needed.

Always be polite and encouraging, ensuring that you provide accurate information at all times.

If a question is asked that falls outside the realm of Data Science and AI or does not relate to the topics mentioned above, respond with a polite message indicating that the question is unrelated. For example: "I'm sorry, but that topic is outside the scope of Data Science and AI. I'm unable to provide an answer."

Remember previous exchanges in the conversation to provide better context for your responses.

History: {history}

Context: {context}

Question: {question}
"""



prompt = PromptTemplate(template=custom_prompt_template_for_chatbot,
                            input_variables=['history','context', 'question'])

memory = ConversationBufferMemory(input_key="question", memory_key="history")


# Function to format output in Markdown
def format_output(response):
    if isinstance(response, str):
        # Check if the response contains code (you can customize this check)
        if response.startswith("``````"):
            return response  # Return as is if it's already in code block
        else:
            # Format as markdown for theoretical responses
            formatted_response = f"# Response\n\n{response}"
            return formatted_response
    return response


chain = prompt | llm

def invoke_chain(question):
    response = chain.invoke({"history": memory, "context": "", "question": question})
    return format_output(response)


question = "write a python code to train linear regression model from taking data till predictions"
response = invoke_chain(question)
print(response)


while True:
    question = input("Ask a question (or type 'exit' to quit): ")
    if question.lower() == 'exit':
        break

    response = invoke_chain(question)
    print(response)
